{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1452a7d2-53c8-4b09-a99d-304b75acd2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.optim import AdamW\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, set_seed\n",
    "from accelerate import Accelerator\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n",
    "from accelerate.utils import ProjectConfiguration\n",
    "import numpy as np \n",
    "from diffusers.optimization import get_scheduler\n",
    "\n",
    "# Configuration\n",
    "output_dir = \"text-inversion-model\"\n",
    "image_path = \"1.png\"  # Path to the single image\n",
    "pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "num_vectors = 1\n",
    "placeholder_token = [\"*\"]\n",
    "resolution = 512\n",
    "train_batch_size = 1\n",
    "max_train_steps = 5000\n",
    "gradient_accumulation_steps = 1\n",
    "learning_rate = 0.0001\n",
    "seed = 42\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(seed)\n",
    "\n",
    "# Initialize the accelerator\n",
    "accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)\n",
    "\n",
    "# Load models\n",
    "vae = AutoencoderKL.from_pretrained(pretrained_model_name_or_path, subfolder=\"vae\", cache_dir=\"model\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_name_or_path, subfolder=\"tokenizer\", cache_dir=\"model\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"text_encoder\", cache_dir=\"model\")\n",
    "unet = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"unet\", cache_dir=\"model\")\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\", cache_dir=\"model\")\n",
    "\n",
    "# Move DDPMScheduler tensors to GPU\n",
    "noise_scheduler.alphas_cumprod = noise_scheduler.alphas_cumprod.to(accelerator.device)\n",
    "# noise_scheduler.alphas_cumprod_prev = noise_scheduler.alphas_cumprod_prev.to(accelerator.device)\n",
    "noise_scheduler.betas = noise_scheduler.betas.to(accelerator.device)\n",
    "\n",
    "# Prepare tokenizer and add placeholder tokens\n",
    "token_ids = tokenizer.encode(placeholder_token, add_special_tokens=False)\n",
    "initializer_token_id = token_ids[0]\n",
    "placeholder_token_ids = tokenizer.convert_tokens_to_ids([initializer_token_id])\n",
    "print(placeholder_token_ids)\n",
    "# Resize the token embeddings as we are adding new special tokens to the tokenizer\n",
    "text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "token_embeds = text_encoder.get_input_embeddings().weight.data\n",
    "\n",
    "with torch.no_grad():\n",
    "    for token_id in placeholder_token_ids:\n",
    "        token_embeds[token_id] = token_embeds[initializer_token_id].clone()\n",
    "\n",
    "# Freeze all parameters except for the token embeddings in text encoder\n",
    "vae.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "text_encoder.text_model.encoder.requires_grad_(False)\n",
    "text_encoder.text_model.final_layer_norm.requires_grad_(False)\n",
    "text_encoder.text_model.embeddings.position_embedding.requires_grad_(False)\n",
    "\n",
    "unet.train()\n",
    "text_encoder.gradient_checkpointing_enable()\n",
    "unet.enable_gradient_checkpointing()\n",
    "\n",
    "# Load and preprocess the image\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((resolution, resolution)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "])\n",
    "image_tensor = preprocess(image).unsqueeze(0).to(accelerator.device)\n",
    "\n",
    "prompt = \"a photo of a \"\n",
    "text_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_index = len(text_input.input_ids[0])\n",
    "print(input_index)\n",
    "text_input = tokenizer(prompt, return_tensors=\"pt\").to(accelerator.device)\n",
    "\n",
    "text_encoder.to(accelerator.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "  text_embeddings_prompt = text_encoder(text_input.input_ids)[0]\n",
    "\n",
    "text_embeddings = torch.randn((1, input_index + 3, 768), dtype=torch.float32, device=accelerator.device, requires_grad=True)\n",
    "text_embeddings.data[:,:input_index,:] = text_embeddings_prompt\n",
    "print(text_embeddings.shape)\n",
    "\n",
    "# Initialize text embeddings with requires_grad=True\n",
    "#text_embeddings = torch.randn((1, 1, 768), dtype = torch.float32, device=accelerator.device, requires_grad=True)\n",
    "# text_embeddings_ssf = torch.load(f'tensor_without_grad_final-lr0.001.pt').to(accelerator.device)\n",
    "text_embeddings_ssf = torch.load(f'1720587980.2415967_average_tensor.pt').to(accelerator.device)\n",
    "\n",
    "text_embeddings.data[:] = text_embeddings_ssf\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [text_embeddings],\n",
    "    lr=learning_rate,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.01,\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     \"cosine\",\n",
    "#     optimizer=optimizer,\n",
    "#     num_warmup_steps=50,\n",
    "#     num_training_steps=max_train_steps,\n",
    "#     num_cycles=1,\n",
    "# )\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"constant\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=80\n",
    ")\n",
    "\n",
    "# Prepare models and optimizer with the accelerator\n",
    "text_embeddings, optimizer, lr_scheduler = accelerator.prepare(text_embeddings, optimizer, lr_scheduler)\n",
    "\n",
    "# Set weight data type based on mixed precision\n",
    "weight_dtype = torch.float32\n",
    "unet.to(accelerator.device, dtype=weight_dtype)\n",
    "vae.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "# Initialize progress bar and original embeddings\n",
    "progress_bar = tqdm(range(max_train_steps), desc=\"Steps\", disable=not accelerator.is_local_main_process)\n",
    "orig_embeds_params = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight.data.clone()\n",
    "losses = []\n",
    "\n",
    "for t in progress_bar:\n",
    "    # Convert image to latent space\n",
    "    target_latents = vae.encode(image_tensor.to(dtype=weight_dtype)).latent_dist.sample().detach()\n",
    "    target_latents = target_latents * vae.config.scaling_factor\n",
    "\n",
    "    with accelerator.accumulate(text_embeddings):\n",
    "        # Sample noise that we'll add to the latents\n",
    "        noise = torch.randn_like(target_latents)\n",
    "        bsz = target_latents.shape[0]\n",
    "        # Sample a random timestep for each image\n",
    "        timestep = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=target_latents.device)\n",
    "        timestep = timestep.long()\n",
    "        \n",
    "        # Add noise to the latents according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        noisy_latents = noise_scheduler.add_noise(target_latents, noise, timestep)\n",
    "              \n",
    "        # noise = noisy_latents - target_latents\n",
    "        \n",
    "        noise_pred = unet(noisy_latents, timestep, encoder_hidden_states=text_embeddings).sample\n",
    "        \n",
    "        # loss = F.mse_loss(latents_pred.float(), target_latents.float(), reduction=\"mean\")\n",
    "        loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\")\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "      \n",
    "        with torch.no_grad():\n",
    "            text_embeddings.grad[:, :input_index,:] = 0  # Zero out, prompt grads\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    losses.append(loss.detach().item())\n",
    "    logs = {\"lr\": lr_scheduler.get_last_lr()[0], \"avg\" : np.mean(losses[-100:])}\n",
    "    \n",
    "    progress_bar.set_postfix(**logs)\n",
    "    \n",
    "    if len(losses) % 50 == 0:\n",
    "        text_embeddings_without_grad = text_embeddings.detach()\n",
    "        \n",
    "        import time\n",
    "        now = time.time()  # Current time\n",
    "\n",
    "        torch.save(text_embeddings_without_grad, os.path.join(\"2024\", str(now) + f'_tensor_without_grad-{len(losses)}-{np.mean(losses[-100:])}.pt'))\n",
    "\n",
    "    if len(losses) % 500 == 0:\n",
    "        torch.save(text_embeddings_without_grad, f'tensor_without_grad-{len(losses)}.pt')\n",
    "\n",
    "text_embeddings_without_grad = text_embeddings.detach()\n",
    "torch.save(text_embeddings_without_grad, f'tensor_without_grad_final-lr{learning_rate}.pt')\n",
    "\n",
    "# Clean up\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "accelerator.end_training()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
