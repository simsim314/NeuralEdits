{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49212368-1cb6-408c-a6db-84e356fae778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline, AutoencoderKL, UNet2DConditionModel, EulerAncestralDiscreteScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection, set_seed\n",
    "from PIL import Image\n",
    "from typing import Any, Dict, Optional\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "class Zero123PlusPipeline(StableDiffusionPipeline):\n",
    "    def __init__(self, vae, text_encoder, tokenizer, unet, scheduler, vision_encoder, feature_extractor_clip, feature_extractor_vae, ramping_coefficients, *args, **kwargs):\n",
    "        super().__init__(vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet, scheduler=scheduler, safety_checker=None, feature_extractor=feature_extractor_vae, *args, **kwargs)\n",
    "\n",
    "        self.vae = vae\n",
    "        self.text_encoder = text_encoder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.unet = unet\n",
    "        self.scheduler = scheduler\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.feature_extractor_clip = feature_extractor_clip\n",
    "        self.feature_extractor_vae = feature_extractor_vae\n",
    "        self.ramping_coefficients = ramping_coefficients\n",
    "        self.depth_transforms_multi = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "\n",
    "    def prepare_unet_call(self, image: Image.Image, prompt: str, num_images_per_prompt: int, depth_image: Image.Image = None):\n",
    "        assert not isinstance(image, torch.Tensor)\n",
    "        \n",
    "        image = self.to_rgb_image(image)\n",
    "        image_1 = self.feature_extractor_vae(images=image, return_tensors=\"pt\").pixel_values\n",
    "        image_2 = self.feature_extractor_clip(images=image, return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "        if depth_image is not None and hasattr(self.unet, \"controlnet\"):\n",
    "            depth_image = self.to_rgb_image(depth_image)\n",
    "            depth_image = self.depth_transforms_multi(depth_image).to(\n",
    "                device=self.unet.controlnet.device, dtype=self.unet.controlnet.dtype\n",
    "            )\n",
    "\n",
    "        image_1 = image_1.to(device=self.vae.device, dtype=self.vae.dtype)\n",
    "        image_2 = image_2.to(device=self.vae.device, dtype=self.vae.dtype)\n",
    "\n",
    "        cond_lat = self.encode_condition_image(image_1)\n",
    "        \n",
    "        if hasattr(self, \"encode_prompt\"):\n",
    "            encoder_hidden_states = self.encode_prompt(prompt, self.device, num_images_per_prompt, False)[0]\n",
    "        else:\n",
    "            encoder_hidden_states = self._encode_prompt(prompt, self.device, num_images_per_prompt, False)\n",
    "\n",
    "        encoded = self.vision_encoder(image_2, output_hidden_states=False)\n",
    "        global_embeds = encoded.image_embeds.unsqueeze(-2)\n",
    "        \n",
    "        ramp = global_embeds.new_tensor(self.ramping_coefficients).unsqueeze(-1)\n",
    "        encoder_hidden_states = encoder_hidden_states + global_embeds * ramp\n",
    "        cross_attention_kwargs = dict(cond_lat=cond_lat)\n",
    "        \n",
    "        if hasattr(self.unet, \"controlnet\"):\n",
    "            cross_attention_kwargs['control_depth'] = depth_image\n",
    "\n",
    "        return image_1, cond_lat, encoder_hidden_states, cross_attention_kwargs\n",
    "        \n",
    "    def encode_condition_image(self, image: torch.Tensor):\n",
    "        image = self.vae.encode(image).latent_dist.sample()\n",
    "        return image\n",
    "        \n",
    "    def run_pipeline(self, image_1: torch.Tensor, cond_lat: torch.Tensor, encoder_hidden_states: torch.Tensor, cross_attention_kwargs: Dict[str, Any], guidance_scale: float, num_images_per_prompt: int, **kwargs):\n",
    "        if guidance_scale > 1:\n",
    "            negative_lat = self.encode_condition_image(torch.zeros_like(image_1))\n",
    "            cond_lat = torch.cat([negative_lat, cond_lat])\n",
    "\n",
    "        latents = super().__call__(\n",
    "            None,\n",
    "            cross_attention_kwargs=cross_attention_kwargs,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            prompt_embeds=encoder_hidden_states,\n",
    "            output_type='latent',\n",
    "            **kwargs\n",
    "        ).images\n",
    "\n",
    "        return latents\n",
    "\n",
    "    def process_output(self, latents: torch.Tensor, output_type: Optional[str] = \"pil\", return_dict: bool = True):\n",
    "        latents = self.unscale_latents(latents)\n",
    "        if output_type != \"latent\":\n",
    "            image = self.unscale_image(self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0])\n",
    "        else:\n",
    "            image = latents\n",
    "\n",
    "        image = self.image_processor.postprocess(image, output_type=output_type)\n",
    "        \n",
    "        if not return_dict:\n",
    "            return image\n",
    "\n",
    "        return ImagePipelineOutput(images=image)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, image: Image.Image = None, prompt: str = \"\", *args, num_images_per_prompt: Optional[int] = 1, guidance_scale: float = 4.0, depth_image: Image.Image = None, output_type: Optional[str] = \"pil\", width: int = 640, height: int = 960, num_inference_steps: int = 28, return_dict: bool = True, **kwargs):\n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image).convert(\"RGB\")\n",
    "\n",
    "        if image is None:\n",
    "            raise ValueError(\"Inputting embeddings not supported for this pipeline. Please pass an image.\")\n",
    "\n",
    "        image_1, cond_lat, encoder_hidden_states, cross_attention_kwargs = self.prepare_unet_call(image, prompt, num_images_per_prompt, depth_image)\n",
    "        latents = self.run_pipeline(image_1, cond_lat, encoder_hidden_states, cross_attention_kwargs, guidance_scale, num_images_per_prompt, **kwargs)\n",
    "        return self.process_output(latents, output_type, return_dict)\n",
    "\n",
    "    @staticmethod\n",
    "    def to_rgb_image(maybe_rgba: Image.Image):\n",
    "        if maybe_rgba.mode == 'RGB':\n",
    "            return maybe_rgba\n",
    "        elif maybe_rgba.mode == 'RGBA':\n",
    "            rgba = maybe_rgba\n",
    "            img = np.random.randint(127, 128, size=[rgba.size[1], rgba.size[0], 3], dtype=np.uint8)\n",
    "            img = Image.fromarray(img, 'RGB')\n",
    "            img.paste(rgba, mask=rgba.getchannel('A'))\n",
    "            return img\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported image type.\", maybe_rgba.mode)\n",
    "\n",
    "    @staticmethod\n",
    "    def unscale_latents(latents):\n",
    "        return (latents / 0.75) + 0.22\n",
    "\n",
    "    @staticmethod\n",
    "    def unscale_image(image):\n",
    "        return image / 0.5 * 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14efd21-3470-4a95-91e3-41a52bcb9f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection, set_seed, CLIPImageProcessor\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, EulerAncestralDiscreteScheduler\n",
    "\n",
    "class Zero123PlusSimSim314Generator(Zero123PlusPipeline):\n",
    "    def __init__(self, config):\n",
    "        self.pipeline_config = config\n",
    "       \n",
    "        vae = AutoencoderKL.from_pretrained(config['pretrained_model_name_or_path'], subfolder=\"vae\").to(self.device)\n",
    "        text_encoder = CLIPTextModel.from_pretrained(config['pretrained_model_name_or_path'], subfolder=\"text_encoder\").to(self.device)\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(config['pretrained_model_name_or_path'], subfolder=\"tokenizer\")\n",
    "        unet = UNet2DConditionModel.from_pretrained(config['pretrained_model_name_or_path'], subfolder=\"unet\").to(self.device)\n",
    "        scheduler = EulerAncestralDiscreteScheduler.from_pretrained(config['pretrained_model_name_or_path'], subfolder=\"scheduler\")\n",
    "        vision_encoder = CLIPVisionModelWithProjection.from_pretrained(config['pretrained_model_name_or_path'], subfolder=\"vision_encoder\").to(self.device)\n",
    "        \n",
    "        feature_extractor_clip = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        feature_extractor_vae = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "        super().__init__(\n",
    "            vae=vae,\n",
    "            text_encoder=text_encoder,\n",
    "            tokenizer=tokenizer,\n",
    "            unet=unet,\n",
    "            scheduler=scheduler,\n",
    "            vision_encoder=vision_encoder,\n",
    "            feature_extractor_clip=feature_extractor_clip,\n",
    "            feature_extractor_vae=feature_extractor_vae,\n",
    "            ramping_coefficients=config[\"ramping_coefficients\"]\n",
    "        )\n",
    "        self.set_seed()\n",
    "\n",
    "    def set_seed(self):\n",
    "        set_seed(self.pipeline_config['seed'])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, image: Image.Image = None, prompt: str = \"\", *args, num_images_per_prompt: Optional[int] = 1, guidance_scale: float = 4.0, depth_image: Image.Image = None, output_type: Optional[str] = \"pil\", width: int = 640, height: int = 960, num_inference_steps: int = 28, return_dict: bool = True, **kwargs):\n",
    "        \n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image).convert(\"RGB\")\n",
    "\n",
    "        if image is None:\n",
    "            raise ValueError(\"Inputting embeddings not supported for this pipeline. Please pass an image.\")\n",
    "\n",
    "        image_1, cond_lat, encoder_hidden_states, cross_attention_kwargs = self.prepare_unet_call(image, prompt, num_images_per_prompt, depth_image)\n",
    "        \n",
    "        self.scheduler.set_timesteps(self.pipeline_config['num_inference_steps'])\n",
    "        latents = self.vae.encode(image_1).latent_dist.sample() * self.scheduler.init_noise_sigma\n",
    "\n",
    "        for i, t in enumerate(tqdm(self.scheduler.timesteps, desc=\"Generating images\")):\n",
    "            latent_model_input = self.scheduler.scale_model_input(latents, t)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs).sample\n",
    "            \n",
    "            latents = self.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                image = self.vae.decode(1 / 0.18215 * latents).sample\n",
    "            \n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "            images = (image * 255).round().astype(\"uint8\")\n",
    "            pil_image = Image.fromarray(images[0])\n",
    "            \n",
    "            output_path = os.path.join(self.pipeline_config['output_dir'], f\"generated_image_{i}.png\")\n",
    "            pil_image.save(output_path)\n",
    "\n",
    "        self.cleanup()\n",
    "        return self.process_output(latents, output_type, return_dict)\n",
    "\n",
    "    def cleanup(self):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "config = {\n",
    "    \"input_image_path\": \"000008_021b957fdc234cf09da4a069cdf57a9b_0.png\",\n",
    "    \"output_dir\": \"path_to_output_dir\",\n",
    "    \"pretrained_model_name_or_path\": \"sudo-ai/zero123plus-v1.1\",\n",
    "    \"resolution\": 512,\n",
    "    \"seed\": 42,\n",
    "    \"guidance_scale\": 1.0,\n",
    "    \"num_inference_steps\": 150,\n",
    "      \"ramping_coefficients\": [\n",
    "    0.0,\n",
    "    0.2060057818889618,\n",
    "    0.18684479594230652,\n",
    "    0.24342191219329834,\n",
    "    0.18507817387580872,\n",
    "    0.1703828126192093,\n",
    "    0.15628913044929504,\n",
    "    0.14174538850784302,\n",
    "    0.13617539405822754,\n",
    "    0.13569170236587524,\n",
    "    0.1269884556531906,\n",
    "    0.1200924888253212,\n",
    "    0.12816639244556427,\n",
    "    0.13058121502399445,\n",
    "    0.14201879501342773,\n",
    "    0.15004529058933258,\n",
    "    0.1620427817106247,\n",
    "    0.17207716405391693,\n",
    "    0.18534132838249207,\n",
    "    0.20002241432666779,\n",
    "    0.21657466888427734,\n",
    "    0.22996725142002106,\n",
    "    0.24613411724567413,\n",
    "    0.25141021609306335,\n",
    "    0.26613450050354004,\n",
    "    0.271847128868103,\n",
    "    0.2850190997123718,\n",
    "    0.285749226808548,\n",
    "    0.2813953757286072,\n",
    "    0.29509517550468445,\n",
    "    0.30109965801239014,\n",
    "    0.31370124220848083,\n",
    "    0.3134534955024719,\n",
    "    0.3108579218387604,\n",
    "    0.32147032022476196,\n",
    "    0.33548328280448914,\n",
    "    0.3301997184753418,\n",
    "    0.3254660964012146,\n",
    "    0.3514464199542999,\n",
    "    0.35993096232414246,\n",
    "    0.3510829508304596,\n",
    "    0.37661612033843994,\n",
    "    0.3913513123989105,\n",
    "    0.42122599482536316,\n",
    "    0.3954688012599945,\n",
    "    0.4260983467102051,\n",
    "    0.479139506816864,\n",
    "    0.4588979482650757,\n",
    "    0.4873477816581726,\n",
    "    0.5095643401145935,\n",
    "    0.5133851170539856,\n",
    "    0.520708441734314,\n",
    "    0.5363377928733826,\n",
    "    0.5661528706550598,\n",
    "    0.5859065651893616,\n",
    "    0.6207258701324463,\n",
    "    0.6560986638069153,\n",
    "    0.6379964351654053,\n",
    "    0.6777164340019226,\n",
    "    0.6589891910552979,\n",
    "    0.7574057579040527,\n",
    "    0.7446827292442322,\n",
    "    0.7695522308349609,\n",
    "    0.8163619041442871,\n",
    "    0.9502472281455994,\n",
    "    0.9918442368507385,\n",
    "    0.9398387670516968,\n",
    "    1.005432367324829,\n",
    "    0.9295969605445862,\n",
    "    0.9899859428405762,\n",
    "    1.044832706451416,\n",
    "    1.0427014827728271,\n",
    "    1.0829696655273438,\n",
    "    1.0062562227249146,\n",
    "    1.0966323614120483,\n",
    "    1.0550328493118286,\n",
    "    1.2108079195022583\n",
    "  ]\n",
    "}\n",
    "\n",
    "os.makedirs(config['output_dir'], exist_ok=True)\n",
    "\n",
    "generator = Zero123PlusSimSim314Generator(config)\n",
    "generator.generate(config['input_image_path'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
